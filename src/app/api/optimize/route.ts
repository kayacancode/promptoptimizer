import { NextRequest, NextResponse } from 'next/server'
import { ConfigFile, OptimizationResult, APIResponse, OptimizationConfig, OptimizationIteration, OptimizationTargets } from '@/types'
import { OptimizationEngine } from '@/lib/optimization'
import { PromptExtractor } from '@/lib/prompt-extractor'
import { ContextAnalyzer } from '@/lib/context-analyzer'
import { LLMProvider } from '@/lib/llm/llm-provider'
import { globalPromptService } from '@/lib/vector-db/global-prompt-service'

const llmProvider = new LLMProvider()

export async function POST(request: NextRequest) {
  try {
    const { configFile, includeContext, useSimpleMode = true, optimizationConfig, userId, useGlobalPrompts = true }: {
      configFile: ConfigFile
      includeContext: boolean
      useSimpleMode?: boolean
      optimizationConfig?: OptimizationConfig
      userId?: string
      useGlobalPrompts?: boolean
    } = await request.json()

    if (!configFile) {
      return NextResponse.json(
        { success: false, error: 'Configuration file is required' } as APIResponse,
        { status: 400 }
      )
    }

    // Extract prompts from code files if not already done
    if (!configFile.extractedPrompts && ['python', 'javascript', 'typescript', 'markdown'].includes(configFile.type)) {
      configFile.extractedPrompts = PromptExtractor.extractPrompts(configFile)
    }

    let optimizationResult: OptimizationResult

    if (useSimpleMode) {
      // Check if iterative optimization is requested
      if (optimizationConfig?.targets || optimizationConfig?.maxIterations) {
        console.log('Using iterative optimization mode with targets...')
        optimizationResult = await performIterativeOptimization(configFile, optimizationConfig, userId, useGlobalPrompts)
      } else {
        // Simple mode: Single API call for optimization with global prompt insights
        console.log('Using simple optimization mode with global prompts...')
        
        const promptToOptimize = configFile.extractedPrompts && configFile.extractedPrompts.length > 0
          ? configFile.extractedPrompts[0].content
          : configFile.content

        // Get global prompt insights if enabled
        let globalInsights = ''
        if (useGlobalPrompts && userId) {
          try {
            await globalPromptService.initialize()
            const insights = await globalPromptService.getPromptInsights(promptToOptimize, userId, {
              limit: 3,
              minScore: 0.6
            })
            
            if (insights.length > 0) {
              globalInsights = `\n\nGlobal Prompt Insights (from ${insights.length} similar prompts):\n` +
                insights.map(insight => `- ${insight.reason} (Score: ${insight.score.toFixed(2)})`).join('\n')
            }
            
            // Add current prompt to global pool
            await globalPromptService.addPromptToGlobalPool(userId, promptToOptimize, {
              type: 'user_prompt',
              complexity: 'medium'
            })
          } catch (error) {
            console.error('Failed to get global prompt insights:', error)
          }
        }

        const response = await llmProvider.generateContent({
          model: 'gemini-2.5-flash',
          max_tokens: 1000,
          temperature: 0.7,
          system: `You are a world-class prompt engineer. The best prompt engineers are clear, iterative, and relentlessly focused on outcomes.

Use this Ultimate Prompt Engineer Checklist to optimize the following prompt:

1. **Clarity and Directness**: Make the prompt unambiguous, specific, and leave no room for misinterpretation. Write instructions as if for a competent outsider with no prior context.

2. **Outcome-Driven**: Evaluate against clear, empirical success criteria. Ensure the prompt matches the intended resultâ€”not just "good enough."

3. **Handle Edge Cases**: Anticipate unusual, malformed, or unexpected inputs. Provide explicit instructions for what to do when the task is unclear or data is missing.

4. **Use Examples Thoughtfully**: When examples are included, make them illustrative, not repetitive. Choose examples that clarify the task, not overfit to narrow patterns.

5. **Leverage Model Strengths**: Trust the model's capabilities, provide rich context when needed. Avoid "babying" the model or dumbing down instructions unnecessarily.

6. **Transparent and Maintainable**: Make the prompt human-readable and easy to debug or adapt. Avoid unnecessary complexity, metaphors, or role-play unless they add clear value.

7. **Empathetic Communication**: "Externalize your brain," making intent legible to both the model and other humans. Imagine yourself as both the model and a naive user.

8. **Focus on Generalization**: Design the prompt to work across a wide range of real-world inputs, not just a curated test set. Avoid overfitting to specific examples.

${globalInsights}

Optimize the following prompt using these principles and incorporate insights from similar high-performing prompts. Return ONLY the optimized prompt without any explanation.`,
          messages: [{
            role: 'user',
            content: promptToOptimize
          }]
        })

        // Calculate confidence based on optimization quality
        const calculateConfidence = (original: string, optimized: string): number => {
          const lengthDifference = Math.abs(optimized.length - original.length) / original.length
          const similarity = original === optimized ? 0 : 1
          const hasSpecificInstructions = /\b(must|should|always|never|exactly|specifically)\b/i.test(optimized)
          const hasExamples = /\bexample|\bfor instance|\be\.g\./i.test(optimized)
          
          let confidence = 0.5 // base confidence
          
          // Boost confidence for meaningful changes
          if (similarity > 0) confidence += 0.2
          if (lengthDifference > 0.1) confidence += 0.1 // significant length change
          if (hasSpecificInstructions) confidence += 0.1
          if (hasExamples) confidence += 0.1
          
          // Boost confidence if global insights were used
          if (globalInsights) confidence += 0.1
          
          return Math.min(0.95, Math.max(0.3, confidence))
        }

        optimizationResult = {
          originalContent: promptToOptimize,
          optimizedContent: response.content,
          explanation: globalInsights ? 
            'Prompt optimized using global prompt insights and proven patterns from similar high-performing prompts.' :
            'Prompt optimized for clarity, specificity, and effectiveness.',
          changes: [{
            type: 'modification',
            line: 1,
            original: promptToOptimize,
            optimized: response.content,
            reason: globalInsights ? 
              'Comprehensive optimization with global prompt insights applied' :
              'Comprehensive optimization applied'
          }],
          confidence: calculateConfidence(promptToOptimize, response.content),
          timestamp: new Date().toISOString()
        }
      }
    } else {
      // Full optimization with context analysis
      let contextInfo = ''
      if (includeContext && configFile.extractedPrompts && configFile.extractedPrompts.length > 0) {
        const contexts = ContextAnalyzer.analyzePromptContext(configFile)
        contextInfo = ContextAnalyzer.generateContextSummary(contexts)
      }

      optimizationResult = await OptimizationEngine.optimize(configFile, {
        includeContext,
        contextInfo,
        userId,
        useGlobalPrompts
      })
    }

    return NextResponse.json({
      success: true,
      data: optimizationResult,
      message: 'Prompt optimization completed successfully'
    } as APIResponse)

  } catch (error) {
    console.error('Optimization error:', error)
    return NextResponse.json(
      {
        success: false,
        error: error instanceof Error ? error.message : 'Optimization failed'
      } as APIResponse,
      { status: 500 }
    )
  }
}

async function performIterativeOptimization(
  configFile: ConfigFile, 
  config: OptimizationConfig,
  userId?: string,
  useGlobalPrompts?: boolean
): Promise<OptimizationResult> {
  const maxIterations = config.maxIterations || 5
  const budget = config.budget || 50
  const costPerIteration = config.costPerIteration || 0.1
  const diminishingReturnsThreshold = config.diminishingReturnsThreshold || 0.02
  const targets = config.targets || {
    overall: 0.85,
    responseQuality: 0.8,
    structureCompliance: 0.85,
    hallucinationRate: 0.15,
    passRate: 0.8
  }

  const promptToOptimize = configFile.extractedPrompts && configFile.extractedPrompts.length > 0
    ? configFile.extractedPrompts[0].content
    : configFile.content

  let currentContent = promptToOptimize
  let totalCost = 0
  const iterationHistory: OptimizationIteration[] = []
  let bestResult: OptimizationResult | null = null
  let lastImprovement = 0

  console.log(`Starting iterative optimization with targets:`, targets)

  for (let iteration = 1; iteration <= maxIterations; iteration++) {
    console.log(`--- Iteration ${iteration} ---`)
    
    if (totalCost + costPerIteration > budget) {
      console.log(`Budget exceeded. Stopping at iteration ${iteration}`)
      break
    }

    // Optimize current content with global insights
    const optimizedContent = await optimizePrompt(currentContent, iteration, targets, userId, useGlobalPrompts)
    totalCost += costPerIteration

    // Create temporary config for evaluation
    const tempConfig = {
      ...configFile,
      content: currentContent
    }

    // Evaluate the optimized prompt
    const evaluationResult = await evaluatePrompt(tempConfig, optimizedContent)
    
    // Calculate improvement
    const improvement = evaluationResult.improvement
    const targetsMet = checkTargetsMet(evaluationResult.afterScore, targets)
    
    console.log(`Iteration ${iteration}: Improvement=${improvement.toFixed(2)}%, Targets met=${targetsMet}`)

    // Store iteration result
    const iterationResult: OptimizationIteration = {
      iteration,
      content: optimizedContent,
      evaluation: evaluationResult,
      targetsMet,
      improvement,
      cost: costPerIteration,
      timestamp: new Date().toISOString()
    }

    iterationHistory.push(iterationResult)

    // Check stopping conditions
    if (targetsMet) {
      console.log(`Targets met at iteration ${iteration}!`)
      iterationResult.stoppingReason = 'targets_met'
      break
    }

    // Check diminishing returns
    if (iteration > 1 && Math.abs(improvement - lastImprovement) < diminishingReturnsThreshold) {
      console.log(`Diminishing returns detected at iteration ${iteration}`)
      iterationResult.stoppingReason = 'diminishing_returns'
      break
    }

    // Update for next iteration
    currentContent = optimizedContent
    lastImprovement = improvement

    // Track best result
    if (!bestResult || evaluationResult.afterScore.overall > bestResult.confidence) {
      bestResult = {
        originalContent: promptToOptimize,
        optimizedContent: optimizedContent,
        explanation: `Iteratively optimized over ${iteration} iterations to meet target metrics.`,
        changes: [{
          type: 'modification',
          line: 1,
          original: promptToOptimize,
          optimized: optimizedContent,
          reason: `Iterative optimization targeting: ${Object.entries(targets).map(([k, v]) => `${k}>${v}`).join(', ')}`
        }],
        confidence: evaluationResult.afterScore.overall,
        iterationHistory,
        timestamp: new Date().toISOString()
      }
    }
  }

  // Mark last iteration if we hit max iterations
  if (iterationHistory.length === maxIterations) {
    iterationHistory[iterationHistory.length - 1].stoppingReason = 'max_iterations'
  }

  // Return best result found
  return bestResult || {
    originalContent: promptToOptimize,
    optimizedContent: currentContent,
    explanation: 'Iterative optimization completed with limited improvements.',
    changes: [{
      type: 'modification',
      line: 1,
      original: promptToOptimize,
      optimized: currentContent,
      reason: 'Iterative optimization attempted'
    }],
    confidence: 0.5,
    iterationHistory,
    timestamp: new Date().toISOString()
  }
}

async function optimizePrompt(content: string, iteration: number, targets: OptimizationTargets, userId?: string, useGlobalPrompts?: boolean): Promise<string> {
  const targetDescription = Object.entries(targets)
    .map(([metric, value]) => `${metric}: ${value}`)
    .join(', ')

  // Get global prompt insights for iterative optimization
  let globalInsights = ''
  if (useGlobalPrompts && userId) {
    try {
      await globalPromptService.initialize()
      const insights = await globalPromptService.getPromptInsights(content, userId, {
        limit: 2,
        minScore: 0.7
      })
      
      if (insights.length > 0) {
        globalInsights = `\n\nGlobal Insights (iteration ${iteration}):\n` +
          insights.map(insight => `- ${insight.reason} (Score: ${insight.score.toFixed(2)})`).join('\n')
      }
    } catch (error) {
      console.error('Failed to get global prompt insights for iteration:', error)
    }
  }

  const response = await llmProvider.generateContent({
    model: 'gemini-2.5-flash',
    max_tokens: 1000,
    temperature: 0.7,
    system: `You are a world-class prompt engineer performing iterative optimization (iteration ${iteration}).

Your goal is to optimize this prompt to meet these specific targets: ${targetDescription}

Focus on:
1. **Clarity and Directness**: Eliminate ambiguity
2. **Outcome-Driven**: Align with target metrics
3. **Handle Edge Cases**: Anticipate unusual inputs
4. **Use Examples Thoughtfully**: Include relevant examples
5. **Leverage Model Strengths**: Trust model capabilities
6. **Transparent and Maintainable**: Keep it readable
7. **Empathetic Communication**: Make intent clear
8. **Focus on Generalization**: Work across inputs

${iteration > 1 ? `This is iteration ${iteration}. Build upon previous improvements while addressing remaining weaknesses.` : ''}
${globalInsights}

Return ONLY the optimized prompt without explanation.`,
    messages: [{
      role: 'user',
      content: content
    }]
  })

  return response.content
}

async function evaluatePrompt(configFile: ConfigFile, optimizedContent: string): Promise<any> {
  try {
    // Call the evaluation API
    const response = await fetch('/api/evaluate', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        originalConfig: configFile,
        optimizationResult: {
          originalContent: configFile.content,
          optimizedContent: optimizedContent,
          explanation: 'Iterative optimization in progress',
          changes: [],
          confidence: 0.5,
          timestamp: new Date().toISOString()
        },
        includeBenchmarks: false,
        skipTestCaseGeneration: false
      })
    })

    const result = await response.json()
    return result.data
  } catch (error) {
    console.error('Evaluation error:', error)
    // Return mock evaluation for fallback
    return {
      beforeScore: {
        structureCompliance: 0.65,
        hallucinationRate: 0.25,
        responseQuality: 0.72,
        overall: 0.67
      },
      afterScore: {
        structureCompliance: 0.75 + Math.random() * 0.15,
        hallucinationRate: 0.20 - Math.random() * 0.05,
        responseQuality: 0.78 + Math.random() * 0.12,
        overall: 0.75 + Math.random() * 0.15
      },
      improvement: 10 + Math.random() * 15,
      testCases: [],
      metrics: {
        totalTests: 0,
        passedTests: 0,
        averageImprovement: 12.5,
        executionTime: 100
      },
      timestamp: new Date().toISOString()
    }
  }
}

function checkTargetsMet(score: any, targets: OptimizationTargets): boolean {
  const checks = [
    !targets.overall || score.overall >= targets.overall,
    !targets.responseQuality || score.responseQuality >= targets.responseQuality,
    !targets.structureCompliance || score.structureCompliance >= targets.structureCompliance,
    !targets.hallucinationRate || score.hallucinationRate <= targets.hallucinationRate,
    !targets.passRate || (score.passRate || 0.8) >= targets.passRate
  ]

  return checks.every(check => check)
}

 